<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Travatar - Evaluation</title>
    <link href="main.css" rel="stylesheet" type="text/css">
</head>
<body>

<div id="all">
<div id="sidebar">

<p>Travatar Main</p>
<ul>
<li><a href="index.html">Home</a></li>
<li><a href="todo.html">Future Plans</a></li>
</ul>

<p>Training</p>
<ul>
<li><a href="training-guide.html">Training Guide</a></li>
<li><a href="preprocessing.html">Preprocessing</a></li>
<li><a href="parsing.html">Parsing</a></li>
<li><a href="training-options.html">Training Options</a></li>
<li><a href="tree-converter.html">Tree Conversion</a></li>
<li><a href="model-format.html">Model Format</a></li>
</ul>

<p>Translating</p>

<ul>
<li><a href="decoding-options.html">Decoding Options</a></li>
<li><a href="evaluation.html">Evaluation</a></li>
</ul>

<p>Other</p>

<ul>
<li><a href="links.html">Links/ Acknowledgements</a></li>
</ul>


</div>
<div id="main">

<h1>MT Evaluation with Travatar</h1>

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#measures">Supported Evaluation Measures</a></li>
<li><a href="#significance">Significance Testing</a></li>
<li><a href="#segmenting">Segmenting Output for Evaluation</a></li>
</ul>

<a name="overview">
<h2>Overview</h2>

<p>
Travatar contains a tool for evaluating translations according to various automatic evaluation metrics.
We do this by passing the reference (<tt>ref.txt</tt>) and system outputs (<tt>sys1.txt</tt> <tt>sys2.txt</tt> or more) to the <tt>mt-evaluator</tt> program.
</p>

<pre>
travatar/src/bin/mt-evaluator -ref ref.txt sys1.txt sys2.txt
</pre>

<p>
This will print scores for BLEU and RIBES.
Normal scores for systems should be a BLEU of around 10-50, and RIBES of around 50-90, with systems on the low end of the range generally being bad and high end of the range being good.
If you would like to get separate evaluation results for every sentence, you can also add <tt>-sent true</tt>.
</p>

<a name="measures">
<h2>Supported Evaluation Measures</h2>

<p>
The evaluation supports several evaluation measures, each of which can optionally be parameterized using a string following a colon:
</p>

<ul>
<li><b>BLEU</b> (indicator string: <tt>bleu:order=4,smooth=0,scope=corpus</tt>)</li>
<li><b>RIBES</b> (indicator string: <tt>ribes:alpha=0.25,beta=0.1</tt>)</li>
<li><b>TER</b> (indicator string: <tt>ter</tt>)</li>
<li><b>WER</b> (indicator string: <tt>wer</tt>)</li>
</ul>

<p>
It is possible to indicate one or more evaluation measures during evaluation using the <tt>-eval</tt> option.
For example, if you want to evaluate BLEU of order 5 and TER, you can call the following command:
</p>

<pre>
travatar/src/bin/mt-evaluator -eval "bleu:order=5 ribes" -ref ref.txt sys1.txt sys2.txt
</pre>

<p>
It is also possible (mainly for tuning) to linearly interpolate two or more evaluation measures using the "interp" pseudo-measure:
</p>

<ul>
<li><b>Interpolated</b> (indicator string: <tt>interp:0.4|bleu:order=5|0.6|ribes</tt>)</li>
</ul>

<a name="significance">
<h2>Significance Testing</h2>

<p>
It is also possible to perform significance testing for differences between the systems using bootstrap resampling.
You can do this by adding <tt>-bootstrap XXX</tt>, where XXX is the number of random samples you will use.
10000 is usually a reasonable number of samples:
</p>

<pre>
travatar/src/bin/mt-evaluator -bootstrap 10000 -ref ref.txt sys1.txt sys2.txt
</pre>

<p>
After the normal evaluation output, you will also get a significance report for each system pair and evaluation measure.
The first column is the fraction of the time that the first system achieves a higher value than the second system, the center value indicates ties, and the final value indicates that the first system gets a lower value.
</p>

<a name="segmenting">
<h2>Segmenting MT Output for Evaluation</h2>

<p>
Sometimes the sentence segmentation of the reference translations and system output does not match, such as in speech translation where sentence boundaries are not clear (<a href="http://mt-archive.info/IWSLT-2005-Matusov.pdf">Matusov et al. 2005</a>).
In this case, you can use the <tt>mt-segmenter</tt> utility to divide the system output into sentences first before evaluation.
Usage is as follows, where <tt>ref.txt</tt> is the reference,  <tt>sys.txt</tt> is the system output (line boundaries are ignored), and <tt>seg.txt</tt> is the segmented output.
</p>

<pre>
travatar/src/bin/mt-segmenter -ref ref.txt sys.txt &gt; seg.txt
</pre>

<p>
By default the segmentation is created to approximately optimize BLEU (with +1 smoothing to make dynamic programming go smoothly), but you can use other optimization measures as listed above.
Also, note that this process is very slow, as it is not using any approximate search techniques.
If you have document boundaries, it would be best to divide along these document boundaries, and send one document at a time to <tt>mt-segmenter</tt>, which will greatly improve speed.
</p>


</div>
</div>

</body>
</html>
