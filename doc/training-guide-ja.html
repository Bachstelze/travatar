<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Travatar - Training</title>
    <link href="main.css" rel="stylesheet" type="text/css">
</head>
<body>

<div id="all">
<div id="sidebar">

<p>Travatar Main</p>
<ul>
<li><a href="index.html">Home</a></li>
<li><a href="todo.html">Future Plans</a></li>
</ul>

<p>Training</p>
<ul>
<li><a href="training-guide.html">Training Guide</a></li>
<li><a href="preprocessing.html">Preprocessing</a></li>
<li><a href="parsing.html">Parsing</a></li>
<li><a href="training-options.html">Training Options</a></li>
<li><a href="tree-converter.html">Tree Conversion</a></li>
<li><a href="model-format.html">Model Format</a></li>
</ul>

<p>Translating</p>

<ul>
<li><a href="decoding-options.html">Decoding Options</a></li>
<li><a href="evaluation.html">Evaluation</a></li>
</ul>

<p>Other</p>

<ul>
<li><a href="links.html">Links/ Acknowledgements</a></li>
</ul>


</div>
<div id="main">

<h1>Travatarの学習</h1>

<p align=right><a href="training-guide.html">English</a></p>

<p>
本ページは、Travatarを使った翻訳モデルの作成方法について説明します。
特に、英日翻訳を例に使いますが、他の言語対でも同様の処理でモデルを作成することができます。
問題や分からないところがあった場合は、気軽に<a href="index.html#development">メインページ</a>に書いてある連絡先へメールをいただければ、お答えします。
</p>

<h2>ソフトのインストール</h2>

<p>
まず、ディレクトリを作って、そこへ移動します。
</p>

<pre>
mkdir ~/travatar-tutorial
cd ~/travatar-tutorial
</pre>

<p>
まず、Travatarの最新版をインストールします。
<a href="index.html#download">ダウンロード</a>ページへ移動し、travatar-tutorialディレクトリ下のtravatarディレクトリへプログラムを入れます。
下記の通り、コマンドを動かして、ヘルプメッセージが表示されればOKです。
</p>

<pre>
cd ~/travatar-tutorial
travatar/src/bin/travatar --help
</pre>

<p>
Travatarに加えて、入力文の構文解析を行う<b>構文解析器</b>、出力文の単語分割（トークン化）を行う<b>トークナイザ</b>、単語の対応付けを行う<b>アライナー</b>が必要です。
本ページでは、英語の構文解析に<a href="http://github.com/odashi/Ckylark">Ckylark</a>、日本語の単語分割に<a href="http://www.phontron.com/kytea">KyTea</a>、単語の対応付けに<a href="http://code.google.com/p/giza-pp/">GIZA++</a>を利用します。
まず、それぞれのサイトに行って、ツールの最新版をダウンロードしてインストールしましょう。
</p>

<p>
Ckylarkに関しては、まずgithubからコードをダウンロードしてから、ディレクトリに入ってプログラムをコンパイルします。
</p>

<pre>
cd Ckylark
autoreconf -i
./configure
make
cd ~/travatar-tutorial
</pre>

<p>KyTeaは下記の通りの手順でコンパイルできます(0.4.7以外のバージョンを使っている場合はバージョン番号を適当に修正してください)。

<pre>
tar -xzf kytea-0.4.7.tar.gz
cd kytea-0.4.7
./configure --prefix=$HOME/travatar-tutorial/usr
make
make install
cd ~/travatar-tutorial
</pre>

<p>
GIZA++のコンパイル手順は下記の通りです。
</p>

<pre>
tar -xzf giza-pp*.tar.gz
cd giza-pp
make
cp GIZA++-v2/GIZA++ GIZA++-v2/*.out mkcls-v2/mkcls .
cd ~/travatar-tutorial
</pre>

<h2>学習データの収集</h2>

<p>
次に、翻訳・言語モデルの学習データが必要です。
本ページでは、<a href="http://www.phontron.com/kftt">京都フリー翻訳タスク</a>のデータを利用します。
下記のコマンドを使ってデータを入手できます。
</p>

<pre>
wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz
tar -xzf kftt-data-1.0.tar.gz
</pre>

<h2>データの準備</h2>

<p>
次に、データをTravatarが使える形へと前処理を行う必要があります。
つまり、入力文の構文解析と、出力文の単語分割を行います。
（これを1回自分でコマンドを動かして行うことをおすすめしますが、手順を理解してから、Travatarに含まれている<a href="preprocessing.html">前処理スクリプト</a>を使うと便利です。）
まず、データディレクトリを作成します。
</p>

<pre>
mkdir data
</pre>

<h3>トークン化</h3>

<p>
まずは、トークン化（つまり単語分割）を行います。
英語に関しては、Travatarに含まれているトークン化プログラムを使います。
</p>

<pre>
travatar/src/bin/tokenizer &lt; kftt-data-1.0/data/orig/kyoto-train.en &gt; data/kyoto-train.tok.en
</pre>

<p>
ここで、<tt>data/kyoto-train.tok.en</tt>を開いてみれば、単語はトークン化されている（つまり単語と句読点の間にスペースが入っている等）ことが分かります。
同じ処理を<tt>kyoto-dev</tt>と<tt>kyoto-test</tt>に対して行います。
</p>

<pre>
travatar/src/bin/tokenizer &lt; kftt-data-1.0/data/orig/kyoto-dev.en &gt; data/kyoto-dev.tok.en
travatar/src/bin/tokenizer &lt; kftt-data-1.0/data/orig/kyoto-test.en &gt; data/kyoto-test.tok.en
</pre>

<p>
次に、日本語をKyTeaでトークン化します。
<tt>-notags</tt>と<tt>-wsconst D</tt>を追加していますが、これは品詞付与を行わなず、数字の分割を抑制するためです。
</p>

<pre>
usr/bin/kytea -notags -wsconst D &lt; kftt-data-1.0/data/orig/kyoto-train.ja &gt; data/kyoto-train.tok.ja
</pre>

<p>
<tt>data/kyoto-train.tok.ja</tt>もきちんとトークン化されていることを確認してから、同じ処理を<tt>kyoto-dev</tt>と<tt>kyoto-test</tt>に対して行います。
</p>

<pre>
usr/bin/kytea -notags -wsconst D &lt; kftt-data-1.0/data/orig/kyoto-dev.ja &gt; data/kyoto-dev.tok.ja
usr/bin/kytea -notags -wsconst D &lt; kftt-data-1.0/data/orig/kyoto-test.ja &gt; data/kyoto-test.tok.ja
</pre>

<h3>学習データから長文の削除</h3>

<p>
学習データに非常に長い文が含まれる時、構文解析や単語アライメントに多くの時間がかかったり、失敗したりする原因になります。
長文をデータから削除するために、Travatarの<tt>clean-corpus.pl</tt>スクリプトを使います。
（<tt>-max_len</tt>の値を変えることで、文の最大長を調整することができます。）
</p>

<pre>
travatar/script/train/clean-corpus.pl -max_len 60 data/kyoto-train.tok.en data/kyoto-train.tok.ja data/kyoto-clean.tok.en data/kyoto-clean.tok.ja
</pre>

<p>
また、通常は翻訳システムを作成する際になるべく多くの学習データを使いたいところですが、今回はチュートリアルなので、学習時間短縮のらめに、最初の2万文のみを使うことにします。
</p>

<pre>
head -20000 &lt; data/kyoto-clean.tok.en &gt; data/kyoto-head.tok.en
head -20000 &lt; data/kyoto-clean.tok.ja &gt; data/kyoto-head.tok.ja
</pre>


<p>
以降のコマンドで<tt>kyoto-head</tt>ではなく、<tt>kyoto-clean<tt/>を利用すると、全データでシステムを学習することが可能です。
</p>

<h3>構文解析</h3>

<p>
次に、Ckylarkを使って、学習データの英文の構文解析を行います。
</p>

<pre>
Ckylark/src/ckylark --add-root-tag --model Ckylark/model/wsj &lt; data/kyoto-head.tok.en &gt; data/kyoto-head.parse.en
</pre>

<p>
<tt>kyoto-dev</tt>と<tt>kyoto-test</tt>に対して同じ処理を行います。
</p>

<pre>
Ckylark/src/ckylark --add-root-tag --model Ckylark/model/wsj &lt; data/kyoto-dev.tok.en &gt; data/kyoto-dev.parse.en
</pre>

<pre>
Ckylark/src/ckylark --add-root-tag --model Ckylark/model/wsj &lt; data/kyoto-test.tok.en &gt; data/kyoto-test.parse.en
</pre>

<p>
構文解析は時間がかかりますので、特に学習データの解析は時間が結構かかります（１〜２時間）。
高速化を含め、構文解析に関する諸事項は<a href="parsing.html">構文解析</a>のページに詳しく書いてあります。
</p>

<h2>言語モデルの学習</h2>

<p>
他の統計的機械翻訳システムと同様に、Travatarは出力の流暢性を担保するための言語モデル(LM)を利用します。
言語モデルを学習するために、まずディレクトリを作成します。
</p>

<pre>
mkdir lm
</pre>

<p>
次に、出力言語のデータを小文字化します。
</p>

<pre>
travatar/script/tree/lowercase.pl &lt; data/kyoto-train.tok.ja &gt; data/kyoto-train.toklow.ja
</pre>

<p>
これに対して、Travatarと同梱されているlmplzを実行します。
</p>

<pre>
travatar/src/kenlm/lm/lmplz -o 5 &lt; data/kyoto-train.toklow.ja &gt; lm/kyoto-train.ja.arpa
</pre>

<p>
出来上がった言語モデルファイルを、読み込みの効率化のために、バイナリ化します。
</p>

<pre>
travatar/src/kenlm/lm/build_binary -i lm/kyoto-train.ja.arpa lm/kyoto-train.ja.blm
</pre>

<h2>翻訳モデルの学習</h2>

<p>
翻訳モデルの学習には、構文解析済みの学習データが必要ですので、構文解析が終わるまで待ちます。
学習データの構文解析が終わったら、まず解析結果に対して小文字化を行います。
（こうすることにより、大文字と小文字の単語を別の単語として扱わないようにします。）
</p>

<pre>
travatar/script/tree/lowercase.pl &lt; data/kyoto-head.parse.en &gt; data/kyoto-head.parselow.en
travatar/script/tree/lowercase.pl &lt; data/kyoto-head.tok.ja &gt; data/kyoto-head.toklow.ja
</pre>

<p>
同じ処理を<tt>kyoto-dev</tt>と<tt>kyoto-test</tt>に対して行います。

<pre>
travatar/script/tree/lowercase.pl &lt; data/kyoto-dev.parse.en &gt; data/kyoto-dev.parselow.en
travatar/script/tree/lowercase.pl &lt; data/kyoto-dev.tok.ja &gt; data/kyoto-dev.toklow.ja
travatar/script/tree/lowercase.pl &lt; data/kyoto-test.parse.en &gt; data/kyoto-test.parselow.en
travatar/script/tree/lowercase.pl &lt; data/kyoto-test.tok.ja &gt; data/kyoto-test.toklow.ja
</pre>

データの準備が終わったら、Travatarの学習スクリプトを動かします。
学習スクリプトは、構文解析済みの英語、トークン化済みの日本語、言語モデル、という３通りの入力を受け取ります。
GIZA++とTravatarのディレクトリを指定し、学習済みのモデルファイルが出力されるディレクトリ<tt>work_dir</tt>を指定します。
学習は時間がかかりますので、&amp;を追加することで背景で動かして、<tt>nohup</tt>を先頭につけます。
また，並列処理が可能なパソコンを使っている場合は、スレッド数を<tt>-threads</tt>コマンドを指定すると、学習が早く終わります。
（特に、下記の通り2以上の数字を指定すると早くなります。）
</p>

<pre>
nohup travatar/script/train/train-travatar.pl -work_dir $HOME/travatar-tutorial/train -lm_file $HOME/travatar-tutorial/lm/kyoto-train.ja.blm -src_file data/kyoto-head.parselow.en -trg_file data/kyoto-head.toklow.ja -travatar_dir travatar -bin_dir giza-pp -threads 2 &amp;&gt; train.log &amp;
</pre>

<p>
学習が終わり次第、<tt>train.log</tt>を見て、学習が無事に終わったことを確認します。
無事に終了した場合は最後に「Finished training!」が書いてあり、何か問題があった場合はだいたいエラーがあります。
エラーが出て、理由が分からない場合は遠慮なく<a href="index.html#development">メインページ</a>の連絡先へtrain.logなどを送って、質問してください。
また、学習には様々なオプションがあり、精度の向上につながることがありますので、1回チュートリアルを一通り終わらせてから<a href="training-options.html">学習オプション</a>のページをご覧下さい。
</p>

<h2>チューニング</h2>

<p>
上記の学習プロセスは翻訳モデルを作成し、一応訳を生成できるようになります。
しかし、良い翻訳精度を実現するために、翻訳モデル・言語モデル・単語ペナルティなどの重みを最適化する<b>チューニング</b>が必要です。
</p>

<p>
重みのチューニングを<tt>mert-travatar.pl</tt>スクリプトで行います。
チューニングの際に、開発データ（<tt>kyoto-dev</tt>）を何回か翻訳する必要があるので、時間がそこそこかかります。
</p>

<pre>
nohup travatar/script/mert/mert-travatar.pl -travatar-config train/model/travatar.ini -nbest 100 -src data/kyoto-dev.parselow.en -ref data/kyoto-dev.toklow.ja -travatar-dir travatar -working-dir tune &amp;&gt; tune.log &amp;
</pre>

<p>
これも、1分以内と早く終了した場合は、問題がある可能性が高いので、<tt>tune.log</tt>を確認してください。
また、複数のコアを使ってチューニングを早くしたい場合は、<tt>-threads XX</tt>オプションを指定します。（XXは使いたいコア数です。）
<p>

<h2>精度評価</h2>

<p>
チューニングが無事に終了すれば、最適化されたモデルは<tt>tune/travatar.ini</tt>に入っているはずです。
これを使って、評価用テキスト<tt>kyoto-test</tt>を翻訳し、精度を評価します。
その前に、モデルファイルが大きくて多くのメモリを必要とするため、翻訳に必要ないルールをあらかじめ削除するようにフィルタリングをかけます。
</p>

<pre>
mkdir test
travatar/script/train/filter-model.pl tune/travatar.ini test/filtered-test.ini test/filtered-test "travatar/script/train/filter-rt.pl -src data/kyoto-test.parselow.en"
</pre>

<p>
ここで、最後の引数はフィルタリングに用いるコマンドです。
<tt>-src</tt>オプションを、翻訳するファイルにします。
フィルタリングが終わり次第、翻訳を行います。
（前と同じく、<tt>-threads XX</tt>を追加することでスピードを向上させることができます。）
</p>

<pre>
travatar/src/bin/travatar -config_file test/filtered-test.ini &lt; data/kyoto-test.parselow.en &gt; test/kyoto-test.out
</pre>

<p>
最後に、翻訳の精度を、自動評価尺度を使って評価します。
参照文とシステム出力を<tt>mt-evaluator</tt>プログラムに入れて、評価します。
他のオプションを<a href="evaluate.html">評価のページ</a>に書いてあります。
</p>

<pre>
travatar/src/bin/mt-evaluator -ref data/kyoto-test.toklow.ja test/kyoto-test.out
</pre>

<p>
これでうまく行ったなら、BLEUが10-12、RIBESが56-58ぐらいの値になります。
精度を更に向上させたい場合は、<a href="parsing.html">構文解析</a>や<a href="training-options.html">学習オプション</a>のページを見ます。
</p>

</div>
</div>

</body>
</html>
