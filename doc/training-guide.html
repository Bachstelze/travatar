<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Travatar - Training</title>
    <link href="main.css" rel="stylesheet" type="text/css">
</head>
<body>

<div id="all">
<div id="sidebar">

<p>Travatar Main</p>
<ul>
<li><a href="index.html">Home</a></li>
<li><a href="todo.html">Future Plans</a></li>
</ul>

<p>Training</p>
<ul>
<li><a href="training-guide.html">Training Guide</a></li>
<li><a href="parsing.html">Parsing</a></li>
<li><a href="training-options.html">Training Options</a></li>
</ul>

<p>Translating</p>

<ul>
<li><a href="decoding-options.html">Decoding Options</a></li>
<li><a href="evaluation.html">Evaluation</a></li>
</ul>

<p>Other</p>

<ul>
<li><a href="links.html">Links/ Acknowledgements</a></li>
</ul>


</div>
<div id="main">

<h1>Training Travatar</h1>

<p>
This guide will take you through the steps required to create a translation model for Travatar.
In particular, we will use English-Japanese translation as an example, but this should work for other languages as well.
If you have any trouble with this tutorial, feel free to ask questions, as mentioned on the <a href="index.html#development">front page</a>.
</p>

<h2>Installing Software</h2>

<p>
First, let's create a directory to work in and change to it:
</p>

<pre>
mkdir ~/travatar-tutorial
cd ~/travatar-tutorial
</pre>

<p>
In addition, you will need to install a <b>syntactic parser</b> to parse the input sentences, a <b>tokenizer</b> for the output sentence, a <b>word aligner</b>, and a <b>language model toolkit</b>.
In this tutorial, in addition to Travatar, we will use the <a href="http://nlp.stanford.edu/software/lex-parser.shtml#Download">Stanford Parser</a> for parsing English, the <a href="http://www.phontron.com/kytea">KyTea word segmenter</a> for segmenting Japanese, and <a href="http://code.google.com/p/giza-pp/">GIZA++</a> for word alignment.
First, go to all of these sites and download the latest versions of the tools to the travatar-tutorial directory and proceed to install.
</p>

<p>
First, you must get the latest version of Travatar.
Follow the directions on the <a href="index.html#download">download</a> page and place the compiled program in the <tt>travatar</tt> directory below <tt>travatar-tutorial</tt>.
If you can run the following commands and get the help message, everythin should be working properly.
</p>

<pre>
cd ~/travatar-tutorial
travatar/src/bin/travatar --help
</pre>

<p>
For the Stanford parser, you must have Java installed on your machine, but as long as you have Java you just need to unzip the archive:
</p>

<pre>
unzip stanford-parser*.zip
</pre>

<p>KyTea can be compiled as follows (if you are not using KyTea 0.4.3, change the version number):</p>

<pre>
tar -xzf kytea-0.4.3.tar.gz
cd kytea-0.4.3
./configure --prefix=$HOME/travatar-tutorial/usr
make
make install
cd ~/travatar-tutorial
</pre>

<p>
GIZA++ can be compiled as follows, and we additionally copy all of the binaries into the top directory for convenience later:
</p>

<pre>
tar -xzf giza-pp*.tar.gz
cd giza-pp
make
cp GIZA++-v2/GIZA++ GIZA++-v2/*.out mkcls-v2/mkcls .
cd ~/travatar-tutorial
</pre>

<h2>Creating a Directory and Collect Data</h2>

<p>
Next, we need to collect data for training the translation and language models.
In this guide, we will use data from the <a href="http://www.phontron.com/kftt">Kyoto Free Translation Task</a>.
You can acquire this data using the following command:
</p>

<pre>
wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz
tar -xzf kftt-data-1.0.tar.gz
</pre>

<h2>Data Preparation</h2>

<p>
The next step is preparing the data in a format so that Travatar's training and translation can work.
This will consist of parsing the input, and tokenizing the output.
First, let's make a directory for the data we will use.
</p>

<pre>
mkdir data
</pre>

<h3>Tokenization</h3>

<p>
The first thing we need to do is tokenize our data (in other words, divide it into words).
For English, or other languages, there is a tokenizer included in the Stanford parser toolkit.
Let's run it on the English training data as follows (we need the 'sed' command because the tokenizer sometimes fails to replace brackets in special cases):
</p>

<pre>
java -cp stanford-parser-2012-11-12/stanford-parser.jar edu.stanford.nlp.process.PTBTokenizer -preserveLines kftt-data-1.0/data/orig/kyoto-train.en | sed 's/(/-LRB-/g; s/)/-RRB-/g' &gt; data/kyoto-train.tok.en
</pre>

<p>
If you take a look at <tt>data/kyoto-train.tok.en</tt> you should see that the words have been tokenized.
Next, we do the same for <tt>kyoto-dev</tt> and <tt>kyoto-test</tt>.
</p>

<p>
Next, we tokenize Japanese with KyTea.
We add <tt>-notags</tt> and <tt>-wsconst D</tt> to suppress the output of POS tags and prevent segmentation of numbers.
</p>

<pre>
usr/bin/kytea -notags -wsconst D &lt; kftt-data-1.0/data/orig/kyoto-train.ja &gt; data/kyoto-train.tok.ja
</pre>

<p>
You can also check to see that the Japanese has been properly segmented into words.
Again, we do the same for <tt>kyoto-dev</tt> and <tt>kyoto-test</tt>.
</p>

<h3>Cleaning the Training Data</h3>

<p>
When very long sentences exist in the training data, they can cause parsing and alignment to take a very long time, or even worse, fail.
To get rid of these sentences from the training data, we use a script included with Travatar to clean the corpus.
(By changing the <tt>-max_len</tt> setting, you can change the maximum length of the data used.)
</p>

<pre>
travatar/script/train/clean-corpus.pl data/kyoto-train.tok.en data/kyoto-train.tok.ja data/kyoto-clean.tok.en data/kyoto-clean.tok.ja
</pre>

<p>
In addition, as you will probably want to go through this tutorial quickly, we will use only some of the training data (e.g. the first 20000 lines).
</p>

<pre>
head -20000 &lt; data/kyoto-clean.tok.en &gt; data/kyoto-head.tok.en
head -20000 &lt; data/kyoto-clean.tok.ja &gt; data/kyoto-head.tok.ja
</pre>


<p>
Note that if you want to actually make a good translation system, you should use all of the data you have.
If you want to do the tutorial with the full data set, just substitute <tt>kyoto-head</tt> into <tt>kyoto-clean</tt> for the rest of the tutorial.
</p>

<h3>Parsing</h3>

<p>
Next, we will use the Stanford parser to parse the source side English sentences.
This can be done with the following command.
</p>

<pre>
java -mx2000m -cp stanford-parser-2012-11-12/stanford-parser.jar:stanford-parser-2012-11-12/stanford-parser-2.0.4-models.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser -tokenized -sentences newline -outputFormat oneline edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz data/kyoto-head.tok.en &gt; data/kyoto-head.parse.en
</pre>

<p>
And do the same for <tt>kyoto-dev</tt> and <tt>kyoto-test</tt>.
</p>

<p>
There are a couple things that should be noted about parsing.
The first is that it takes a long time to parse many sentences, which can be solved to some extent by using multiple cores or multiple machines.
The second is that while the Stanford parser is robust and easy to use, there are other parsers that can achieve better accuracy than the Stanford parser.
These are highly recommended if you want to improve Travatar's translation accuracy, as changing parsers can result in a very significant gain of up to several BLEU points.
Both of these issues are discussed in more detail on the <a href="parsing.html">parsing page</a>.
</p>

<h2>Training the Language Model</h2>

<p>
As with most statistical translation systems, Travatar can use a language model (LM) to improve the fluency of its output.
In order to train the LM, we first make a language model directory:
</p>

<pre>
mkdir lm
</pre>

<p>
Then run a series of three commands that add start and finish tokens to the training data,
</p>

<pre>
travatar/script/tree/lowercase.pl &lt; data/kyoto-train.tok.ja &gt; data/kyoto-train.toklow.ja
</pre>
<pre>
irstlm-5.80.01/scripts/add-start-end.sh &lt; data/kyoto-train.toklow.ja &gt; lm/kyoto-train.startend.ja
</pre>

<p>
and build a language model:
</p>

<pre>
IRSTLM=$HOME/travatar-tutorial/usr/ irstlm-5.80.01/scripts/build-lm.sh -i lm/kyoto-train.startend.ja  -t ./tmp -p -s improved-kneser-ney -n 5 -o lm/kyoto-train.ja.lm
</pre>

<p>
Here, <tt>IRSTLM</tt> indicates the install path we used when we installed IRSTLM, <tt>-i</tt> indicates our training data  <tt>./tmp</tt> is a directory for temporary files,  <tt>-s</tt> is our smoothing method, <tt>-n</tt> is the order of the language model, and <tt>-o</tt> is our output model.
Finally, we compile the model to ARPA format,
</p>

<pre>
irstlm-5.80.01/src/compile-lm --text yes lm/kyoto-train.ja.lm.gz lm/kyoto-train.ja.arpa
</pre>

<p>
and binarize it for faster loading:
</p>

<pre>
travatar/src/kenlm/lm/build_binary -i lm/kyoto-train.ja.arpa lm/kyoto-train.ja.blm
</pre>

<h2>Training the Translation Model</h2>

<p>
Training the translation model requires the parsed training data, so you have to wait until the parsing is finished, at least for the training set.
In order to prevent lower case words and upper case words from being treated differently, we will first want to convert all the data to lower case:
</p>

<pre>
travatar/script/tree/lowercase.pl &lt; data/kyoto-head.parse.en &gt; data/kyoto-head.parselow.en
travatar/script/tree/lowercase.pl &lt; data/kyoto-head.tok.ja &gt; data/kyoto-head.toklow.ja
</pre>

<p>
And do the same for <tt>kyoto-dev</tt> and <tt>kyoto-test</tt>.
Once this data is prepared, we run the following training script.
Note that this takes our parsed English, tokenized Japanese, and language model as input.
We specify the directories for GIZA++ and Travatar, and also our working directory, where the model will be stored.
This will take a little while, so we will run it in the background using <tt>nohup</tt> at the beginning and <tt>&amp;</tt> at the end.
In addition, if you have a computer with multiple cores, you can specify the number of cores you would like to use with <tt>-threads</tt> (for example, with 2 threads below).
</p>

<pre>
nohup travatar/script/train/train-travatar.pl -work_dir $HOME/travatar-tutorial/train -lm_file $HOME/travatar-tutorial/lm/kyoto-train.ja.blm -src_file data/kyoto-head.parselow.en -trg_file data/kyoto-head.toklow.ja -travatar_dir travatar -bin_dir giza-pp -threads 2 &amp;&gt; train.log &amp;
</pre>

<p>
If training ends very quickly, there is probably something wrong, so check <tt>train.log</tt> for any error messages.
There are a couple of <a href="training-options.html">options for training</a> that can improve the accuracy of translation, so once you have gone through the tutorial it will be worth checking them out.
</p>

<h2>Tuning</h2>

<p>
The above training creates the fundamental translation model, and we are able to perform translation.
However, to achieve reasonable accuracy, we must perform tuning, which adjusts the weights of the translation model, language model, word penalties, etc.
</p>

<p>
This is done with the <tt>mert-travatar.pl</tt> script in the following fashion.
This also takes a considerable amount of time, as we have to translate the development set several times.
</p>

<pre>
nohup ~/work/travatar/script/mert/mert-travatar.pl -travatar-config train/model/travatar.ini -nbest 100 -src data/kyoto-dev.parselow.en -ref data/kyoto-dev.toklow.ja -travatar-dir travatar -working-dir tune &amp;&gt; tune.log &amp;
</pre>

<p>
Again, if this finishes very quickly, there is probably an error in <tt>tune.log</tt>.
Also, if you want to speed up the tuning process you can use multiple processors by adding <tt>-decoder-options "-threads XX"</tt> where XX is the number of processors to use.
</p>

<h2>Testing</h2>

<p>
When tuning finishes, there will be an appropriately tuned model in <tt>tune/travatar.ini</tt>.
We can now use this model to translate the test text using the Travatar decoder.
Before training, we will want to filter the model file to remove rules that are not needed for translation and reduce the memory footprint:
</p>

<pre>
mkdir test
travatar/script/train/filter-model.pl tune/travatar.ini test/filtered-test.ini test/filtered-test "travatar/script/train/filter-rt.pl -src data/kyoto-test.parselow.en"
</pre>

<p>
Here the final argument in quotes is what command we will use to filter the rule table.
You should change the <tt>-src</tt> option to whatever file you will be translating.
Once we are done filtering, we can translate the test set as follows (again add <tt>-threads XX</tt> to speed things up):
</p>

<pre>
travatar/src/bin/travatar -config_file test/filtered-test.ini &lt; data/kyoto-test.parselow.en &gt; test/kyoto-test.out
</pre>

<p>
Finally, we can measure the accuracy of our translations using an automatic evaluation metric.
We do this by passing the reference and system output to the <tt>mt-evaluator</tt> program included with Travatar (<a href="evaluate.html">other options</a>).
</p>

<pre>
travatar/src/bin/mt-evaluator -ref data/kyoto-test.toklow.ja test/kyoto-test.out
</pre>

<p>
If everything went OK, you should get a BLEU of around 10-12, RIBES of around 56-58 with the smaller data, or more with the full training set.
If you want to improve the accuracy even more, please be sure to visit the <a href="parsing.html">parsing</a> or <a href="training-options.html">training options</a> sections for more details!
</p>

</div>
</div>

</body>
</html>
