<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Travatar - Preprocessing for Travatar</title>
    <link href="main.css" rel="stylesheet" type="text/css">
</head>
<body>

<div id="all">
<div id="sidebar">

<p>Travatar Main</p>
<ul>
<li><a href="index.html">Home</a></li>
<li><a href="todo.html">Future Plans</a></li>
</ul>

<p>Training</p>
<ul>
<li><a href="training-guide.html">Training Guide</a></li>
<li><a href="preprocessing.html">Preprocessing</a></li>
<li><a href="parsing.html">Parsing</a></li>
<li><a href="training-options.html">Training Options</a></li>
<li><a href="tree-converter.html">Tree Conversion</a></li>
</ul>

<p>Translating</p>

<ul>
<li><a href="decoding-options.html">Decoding Options</a></li>
<li><a href="evaluation.html">Evaluation</a></li>
</ul>

<p>Other</p>

<ul>
<li><a href="links.html">Links/ Acknowledgements</a></li>
</ul>


</div>
<div id="main">

<h1>Preprocessing for Travatar</h1>

<p>
As Travatar requires parsing, the preprocessing is a bit heavier than that necessary for other translation toolkits.
There is some information on the <a href="parsing.html">Parsing</a> page about how to do this for each language.
However, there are a lot of fine details and it is easy to mess things up, so we provide a default preprocessing script that supports English (en), Japanese (ja), Chinese (zh), German (de), and French (fr).
To use it follow the directions below:
</p>

<h2>Installing Programs</h2>

<p>
The first thing that you will need to do is install programs other than Travatar that are needed to parse your particular language pair.
The best place to install these programs is in <tt>$HOME/usr/local</tt>.
If you install them in a different place you can specify them using <tt>-program-dir /path/to/tools</tt> when you run the preprocessing script.
</p>

<ul>
<li><a href="http://code.google.com/p/giza-pp">GIZA++ Aligner</a> (all): Unzip, enter the <tt>giza-pp</tt> directory, <tt>make</tt>, and then run the following command:
<pre>
cp GIZA++-v2/{GIZA++,plain2snt.out,snt2cooc.out,snt2plain.out,trainGIZA++.sh} mkcls-v2/mkcls .
</pre>
</li>

<li><a href="http://nlp.stanford.edu/software/lex-parser.shtml">Stanford Parser</a> (en, zh, de, fr): Unzip, then run the following commands to normalize version numbers:
<pre>
ln -s `pwd`/stanford-parser-* stanford-parser
cd stanford-parser
ln -s `pwd`/stanford-parser-*-models.jar stanford-parser-models.jar
</pre>
</li>

<li><a href="http://github.com/neubig/egret">Egret Parser</a> (en, zh): Unzip, run <tt>bash make-linux.sh</tt>.</li>

<li><a href="http://phontron.com/kytea">KyTea Morphological Analyzer</a> (ja): Unzip, enter the directory, <tt>./configure</tt>, <tt>make</tt> and <tt>sudo make install</tt>.
</li>

<li><a href="http://plata.ar.media.kyoto-u.ac.jp/tool/EDA">Eda Parser</a> (ja): Unzip, enter the directory, <tt>make</tt>.</li>

<li><a href="http://code.google.com/p/nile/">Nile</a> (optional): Requires several libraries, etc. Follow the install directions.</li>
</ul>

<h2>Running Preprocessing</h2>

<h3>Preprocessing Training Data</h3>

<p>
If you have all of the tools installed in the <tt>$HOME/usr/local</tt> directory, running the preprocessing script is quite simple.
For example, considering translation between English and Japanese, we could run the following command to tokenize, clean, parse, and align our training data:
</p>

<pre>
$TRAVATAR_DIR/script/preprocess/preprocess.pl
   -src en -trg ja -threads 4 -clean 60 -align train.en train.ja preproc-train
</pre>

<p>
where <tt>train.en</tt> is the source file, <tt>train.ja</tt> is the target file, and <tt>preproc-train</tt> is the output directory.
<tt>-threads</tt> indicates the number of threads you want to use (if you use more, preprocessing will finish more quickly), <tt>-clean 60</tt> indicates you want to remove all sentences of more than 60 words, and <tt>-align</tt> indicates that you want to run GIZA++ to generate alignments.
When preprocessing finishes, you will have a number of directories under <tt>preproc-train</tt> containing the corpus in various forms.
</p>

<p>
In addition, if you don't mind a small amount of extra work to achieve the best accuracy possible, you can use Nile to obtain better word alignments.
This can be done by adding "<tt>-nile-model MODEL_NAME.txt</tt>" where <tt>MODEL_NAME.txt</tt> is the name of a model for Nile that you have trained according to the Nile directions.
You can download a model for the following language pairs (if you have a model for a different language pair, please contribute! We will list it here):
</p>
<ul>
<li>English-Japanese (<a href="http://www.phontron.com/travatar/download/nile-en-ja.model">en-ja</a>)</li>
</ul>
<p>
If the model is in the opposite direction as the source/target in the preprocessing script, add the "<tt>-nile-direction trgsrc</tt>" option.
</p>


<h3>Preprocessing Testing Data</h3>

<p>
When preprocessing your development and testing data, you might want to use a command like this:
</p>

<pre>
$TRAVATAR_DIR/script/preprocess/preprocess.pl
   -src en -trg ja -threads 4 -forest-src dev.en dev.ja preproc-dev
$TRAVATAR_DIR/script/preprocess/preprocess.pl
   -src en -trg ja -threads 4 -forest-src test.en test.ja preproc-test
</pre>

<p>
In this case we should not filter the data and don't need to align, but we may want to generate forests for the languages that support them (e.g. en, zh).
</p>

<h2>Training/Tuning/Testing</h2>

<p>
Once we have run the preprocessing script, it is pretty easy to run training, tuning, and testing.
For example, to build an English-Japanese system we first run training (assuming we've already built a language model <tt>ja.blm</tt> as mentioned on the <a href="training-guide.html">training guide</a> page):
</p>
<pre>
nohup travatar/script/train/train-travatar.pl -work_dir train -lm_file ja.blm -src_file preproc-train/treelow/en -trg_file preproc-train/low/ja -align-file preproc-train/giza/enja -travatar_dir travatar -threads 2 &amp;&gt; train.log &amp;
</pre>
<p>
Next, we perform tuning, using the forests (hence <tt>-in-format egret</tt>) we created for the dev set:
</p>
<pre>
nohup ~/work/travatar/script/mert/mert-travatar.pl -in-format egret -travatar-config train/model/travatar.ini -src preproc-dev/forlow/en -ref preproc-dev/low/ja -travatar-dir travatar -working-dir tune &amp;&gt; tune.log &amp;
</pre>
<p>
Finally, we decode using the tuned model:
</p>
<pre>
travatar/src/bin/travatar -config_file tune/travatar.ini -in_format egret &lt; preproc-test/forlow/en &gt; output.ja
</pre>
<p>
and measure the accuracy:
</p>
<pre>
travatar/src/bin/mt-evaluator -ref preproc-test/low/ja output.ja
</pre>

</div>
</div>

</body>
</html>
